# -*- coding: utf-8 -*-
"""DSPPreprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gRIkHW4QwOkpFlkZUMWtueG11ZcQg881

# DATA PREPROCESSING

# Installing Libraries

- Contraction is used to process the raw data (text), by expanding the contraction (e.g.,"can't" to "cannot" or "it's" to "it is"), making text cleaner and more consistent for analysis.
- Pandas is used for analyzing and cleaning the dataset.
- Matplotlib, seaborn and wordcloud, are use to visualize trends and patterns.
"""

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from datetime import datetime

"""## Installing libraries for NLP process"""

import re # Regular, to peform text cleaning (remove urls, special characters and extra spaces)
import nltk # NLTK is used for NLP process mainly in text preprocessing (Tokenization, stopword removal, lemmatization, POS taagging)

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk import pos_tag
from contractions import fix #To expand contracted words like "can't" to "cannot".

# Download necessary NLTK data
nltk.download('punkt_tab') # for tokenizing sentences or words
nltk.download('punkt')
nltk.download('wordnet') # For Wordnet, used in lemmatization to access synonyms, definations, and for lemmatization
nltk.download('omw-1.4')  # WordNet multilingual extensions, to support multilingual
nltk.download('averaged_perceptron_tagger_eng') # For part-of-speech tagging
nltk.download('stopwords') # For stopword removal during text preprocessing

# Load the Smartphone dataset
file_path = 'Dataset\\Apple and Samsung Smartphones Dataset2.csv'
df = pd.read_csv(file_path)

# Display basic infor of Smartphone dataset
print("Dataset Overview:")
print(df.info())
print("\nSample Data:")
print(df.head())
num_tweets = len(df)
print(f"Number of tweets: {num_tweets}")

"""## Handling Missing Values & Duplicates"""

# Check for missing values
print("\nMissing Data:")
print(df.isnull().sum())

# Drop rows with missing values in critical columns (if any)
df = df.dropna(subset=['full_text', 'created_at'])
# Drop duplicates (if any)
df = df.drop_duplicates()

# Number of rows after dropping
num_tweets = len(df)
print(f"Number of tweets after dropping: {num_tweets}")

"""## Text preprocessing

1.   Mentions (@username) are removed because username is not necessary in this sentiment analysis project
2.   Contractions (e.g., "can't" â†’ "cannot") are expanded using the contractions.fix() function
3.   URLs are removed using regular expressions
4.   Special characters and punctuation are removed, keeping only alphanumeric characters and dots (___)
5.   The text is converted to lowercase
6.   Remove extra whitespace to ensure consistent spacing
7.   The text is tokenized into individual words using word_tokenize() and assign POS tags using pos_tag(token)
7.   Lemmatized using WordNetLemmatizer, into their base forms using POS tags, excluding custom stopwords and non-lemmatizable words.
8.   The processed tokens are joined back into a single string
"""

# List of custom stopwords to retain from removing, including "no" and "not"
default_stopwords = set(stopwords.words('english'))
custom_stopwords = default_stopwords - {'no', 'not', 'nor', 'none', 'never', 'neither', 'without', 'against',
                                        'but', 'however', 'though', 'although',
                                        'because', 'since', 'due to', 'with'}


# Words that should not be lemmatized
non_lemmatizable_words = {'iphone16', 'iphone16plus', 'iphone16pro', 'iphone16promax', "ios", 'iphone15', 'iphone15plus', 'iphone15pro', 'iphone15promax',
                          'samsunggalaxy23', 'samsunggalaxys23plus', 'samsunggalaxys23ultra', 'samsunggalaxy24', 'samsunggalaxys24plus', 'samsunggalaxys24ultra',
                          'ios17', 'ios18', 'dynamic island', 'a17bionic', 'a18chip', 'usb-c', 'lightning port', 'pro motion', 'ceramic shield',
                          'snapdragon', 'exynos', '120hz', 'amozed', 'one ui',
                          '5g', 'refresh rate', 'fast charging', 'screen size'
                          }

# Function to map POS tags to WordNet tags
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # Default to noun

# Function for text preprocessing
def preprocess_text(text):
    # Remove mentions
    text = re.sub(r'@\w+', '', text)
    # Expand contractions (assuming `fix()` function is defined elsewhere)
    text = fix(text)
    # Remove URLs
    text = re.sub(r'http\S+|www\.\S+', '', text)
    # Remove special characters and punctuation, but keep numbers and dots
    text = re.sub(r'[^a-zA-Z0-9.\s]', '', text)
    # Convert to lowercase
    text = text.lower()
    # Remove redundant whitespaces
    text = re.sub(r'\s+', ' ', text).strip()

    # Tokenize and POS tag
    tokens = word_tokenize(text)
    pos_tags = pos_tag(tokens)
    lemmatizer = WordNetLemmatizer()

    # Enhanced: Edge case handling for empty text
    if not tokens:
        return ''  # Return an empty string if no valid tokens remain

    # Processing tokens with enhanced handling
    processed_tokens = [
        lemmatizer.lemmatize(word, get_wordnet_pos(tag))
        if word.isalpha() and word not in custom_stopwords and word not in non_lemmatizable_words
        else word
        for word, tag in pos_tags
    ]

    return ' '.join(processed_tokens)

# Apply preprocessing to `full_text`
df['cleaned_text'] = df['full_text'].astype(str).apply(preprocess_text)
df = df.drop(columns=['created_at'])

"""- The preprocess_text function is applied to the full_text column, and the cleaned text is stored in a new column, cleaned_text."""

sample_text = "Samsung s24 is cheaper than iphone 16 pro max"
print(preprocess_text(sample_text))

# Display preprocessed data
print("\nSample of Preprocessed Data:")
print(df[['full_text', 'cleaned_text']].head())

"""# Step 2: Exploratory Data Analysis (EDA)

## Summarizing favorite, reply and retweet counts columns
"""

# Summarize analysis of favorite, reply, and retweet counts
summary_stats = df[['favorite_count', 'reply_count', 'retweet_count']].describe()
print("Summary Statistics:")
print(summary_stats)

# Boxplots for engagement metrics
plt.figure(figsize=(12, 6))
plt.subplot(1, 3, 1)
sns.boxplot(y=df['favorite_count'], color='skyblue')
plt.title('Favorite Count Distribution')

plt.subplot(1, 3, 2)
sns.boxplot(y=df['reply_count'], color='lightgreen')
plt.title('Reply Count Distribution')

plt.subplot(1, 3, 3)
sns.boxplot(y=df['retweet_count'], color='lightcoral')
plt.title('Retweet Count Distribution')

plt.tight_layout()
plt.show()

# Correlation matrix
plt.figure(figsize=(8, 6))
sns.heatmap(df[['favorite_count', 'retweet_count', 'reply_count']].corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

"""Here are the insights got from exploratory numerical columns, favorite, reply and retweet count columns.

- Understanding Engagement Levels of smartphone by reflecting the number of these 3 columns to the text.
- Identifying min, max, average and outliers for each numerical columns to identify trends and anomalies

## Smartphones EDA
"""

# Temporal analysis: Tweets by year and month
year_month_counts = df.groupby(['Year', 'Month']).size().reset_index(name='tweet_count')

plt.figure(figsize=(12, 6))
sns.lineplot(data=year_month_counts, x='Month', y='tweet_count', hue='Year', marker='o')
plt.title('Tweet Activity by Month and Year')
plt.xlabel('Month')
plt.ylabel('Tweet Count')
plt.legend(title='Year')
plt.show()

# Smartphone-specific tweet analysis (Year and Month)
tweets_by_smartphone = df.groupby(['smartphone', 'Year', 'Month']).size().reset_index(name='tweet_count')

# Combine Year and Month into a single column for easier plotting
tweets_by_smartphone['Year-Month'] = tweets_by_smartphone['Year'].astype(str) + '-' + tweets_by_smartphone['Month'].astype(str)

plt.figure(figsize=(16, 8))
sns.barplot(data=tweets_by_smartphone, x='Year-Month', y='tweet_count', hue='smartphone')
plt.title('Monthly Tweet Activity by Smartphone and Year')
plt.xlabel('Year-Month')
plt.ylabel('Tweet Count')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility
plt.legend(title='Smartphone Model')
plt.tight_layout()
plt.show()

"""- Data Validation
> To help me ensure the dataset is accurate or not based on month and year of their launching and purchasing the products.

- Trend Analysis
> To identify trends of monthly tweet activity by smartphones. Peak in activity may correspond to product launches, announcement, events and comparison with between models or different smartphones.


"""

# Group by smartphone and count unique conversation IDs
model_counts = df.groupby('smartphone')['conversation_id_str'].size().reset_index(name='tweet_count')
print(model_counts)

# Plotting bar chart
plt.figure(figsize=(10, 6))
sns.barplot(data=model_counts, x='smartphone', y='tweet_count', palette='viridis')
plt.title('Number of Tweets for Each Smartphone Model')
plt.xlabel('Smartphone Model')
plt.ylabel('Number of Tweets')
plt.xticks(rotation=45)
plt.show()

"""- Understanding the popularity each smartphone

> iPhone 15 is the most disccused model with 3539 tweet count, nearly double the tweet count of iPhone16.

> Samsung s24 has slightly more engagement than s23, reflecting user interest in the newer falgship

> Apple models collectively have higher tweet count than samsung, potetially reflecting stronger social media engagement, due to its controversional, and loyalty customers.

> Iphone 16 and Samsung s24 are the new model launched by 2024, so they may need more time to generate buzz.
"""

# Word cloud generation
def generate_wordcloud(data, title):
    text = " ".join(data)
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title, fontsize=16)
    plt.axis('off')
    plt.show()

# Generate word clouds for iPhone and Samsung
dice_iphone = df[df['smartphone'].str.contains('iPhone', case=False, na=False)]
dice_samsung = df[df['smartphone'].str.contains('Samsung', case=False, na=False)]

print("Generating word clouds...")
generate_wordcloud(dice_iphone['cleaned_text'], 'Word Cloud for iPhone Tweets')
generate_wordcloud(dice_samsung['cleaned_text'], 'Word Cloud for Samsung Tweets')

"""# Visualize keyword counts"""

# Step 5: Sentiment Analysis Preparation
negative_keywords = ['annoying', 'bad', 'broken', 'buggy', 'complicated', 'disappointing', 'disliked', 'expensive',
                     'frustrated', 'frustrating', 'glitchy', 'hated', 'irritating', 'laggy', 'lacking', 'not worth',
                     'outdated', 'overheating', 'overhyped', 'overpriced', 'poor', 'problematic', 'slow', 'terrible',
                     'unacceptable', 'unimpressed', 'unreliable', 'unsatisfied', 'unstable', 'unresponsive', 'useless',
                     'weak', 'worst']

positive_keywords = ['affordable', 'amazing', 'awesome', 'beautiful', 'best', 'elegant', 'excellent', 'exceptional',
                     'fantastic', 'fast', 'favorite', 'flawless', 'great', 'iconic', 'impressive', 'incredible',
                     'innovative', 'love', 'next-level', 'outstanding', 'perfect', 'powerful', 'premium', 'reliable',
                     'revolutionary', 'smooth', 'stylish', 'superior', 'top-notch', 'unique', 'user-friendly', 'worth']

character_keywords = ['5g', 'ios17', 'ios18', 'a17bionic', 'a18chip', 'accessibility', 'battery', 'battery health', 'bezel', 'bluetooth', 'brightness',  'bugs', 'build quality', 'camera', 'cellular', 'charging', 'chipset', 'cinematic mode', 'colors',  'comparison', 'customization', 'design', 'display', 'dual sim', 'dynamic island', 'ecosystem',  'endurance', 'esim', 'fast charging', 'features', 'gaming', 'integration', 'ios', 'mag safe',  'material', 'multitasking', 'night mode', 'notch', 'oled', 'operating system', 'performance',  'port', 'portrait mode', 'price', 'processor', 'ram', 'refresh rate', 'resolution', 'size',  'software', 'speed', 'stabilization', 'storage', 'updates', 'video quality', 'vs android',  'vs samsung', 'weight', 'wi-fi', 'wireless charging', 'zoom']

apple_keywords = ['iphone16', 'iphone16plus', 'iphone16pro', 'iphone16promax', 'iphone15', 'iphone15plus', 'iphone15pro', 'iphone15promax']
samsung_keywords = ['SamsungGalaxyS23', 'SamsungGalaxyS23Plus', 'SamsungGalaxyS23Ultra', 'SamsungGalaxyS24', 'SamsungGalaxyS24Plus', 'SamsungGalaxyS24Ultra']

# Pre-defined apple_df and samsung_df
apple_df = df[df['cleaned_text'].str.contains('Apple', case=False, na=False)]
samsung_df = df[df['cleaned_text'].str.contains('Samsung', case=False, na=False)]

for keyword in negative_keywords + positive_keywords + character_keywords:
    apple_df[keyword] = apple_df['cleaned_text'].str.contains(keyword, case=False, na=False)
    samsung_df[keyword] = samsung_df['cleaned_text'].str.contains(keyword, case=False, na=False)

# Calculate keyword counts for Apple
apple_negative_counts = apple_df[negative_keywords].sum().sort_values(ascending=False)
apple_positive_counts = apple_df[positive_keywords].sum().sort_values(ascending=False)
apple_character_counts = apple_df[character_keywords].sum().sort_values(ascending=False)

# Calculate keyword counts for Samsung
samsung_negative_counts = samsung_df[negative_keywords].sum().sort_values(ascending=False)
samsung_positive_counts = samsung_df[positive_keywords].sum().sort_values(ascending=False)
samsung_character_counts = samsung_df[character_keywords].sum().sort_values(ascending=False)

# Plot Apple graphs
plt.figure(figsize=(10, 6))
sns.barplot(x=apple_negative_counts.values, y=apple_negative_counts.index, palette='cool')
plt.title('Negative Keyword Occurrences for Apple')
plt.xlabel('Count')
plt.ylabel('Keyword')
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(x=apple_positive_counts.values, y=apple_positive_counts.index, palette='cool')
plt.title('Positive Keyword Occurrences for Apple')
plt.xlabel('Count')
plt.ylabel('Keyword')
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(x=apple_character_counts.values, y=apple_character_counts.index, palette='cool')
plt.title('Character Keyword Occurrences for Apple')
plt.xlabel('Count')
plt.ylabel('Keyword')
plt.show()

print("\nCharacter Keyword Counts for Apple:")
print(apple_character_counts)

print("Negative Keyword Counts for Apple:")
print(apple_negative_counts)

print("\nPositive Keyword Counts for Apple:")
print(apple_positive_counts)

# Plot Samsung graphs
plt.figure(figsize=(10, 6))
sns.barplot(x=samsung_negative_counts.values, y=samsung_negative_counts.index, palette='cool')
plt.title('Negative Keyword Occurrences for Samsung')
plt.xlabel('Count')
plt.ylabel('Keyword')
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(x=samsung_positive_counts.values, y=samsung_positive_counts.index, palette='cool')
plt.title('Positive Keyword Occurrences for Samsung')
plt.xlabel('Count')
plt.ylabel('Keyword')
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(x=samsung_character_counts.values, y=samsung_character_counts.index, palette='cool')
plt.title('Character Keyword Occurrences for Samsung')
plt.xlabel('Count')
plt.ylabel('Keyword')
plt.show()

print("\nCharacter Keyword Counts for Samsung:")
print(samsung_character_counts)

print("Negative Keyword Counts for Samsung:")
print(samsung_negative_counts)

print("\nPositive Keyword Counts for Samsung:")
print(samsung_positive_counts)

"""# LABELLING DATA WITH VADER"""

# Import the Necessary Libraries
from nltk.sentiment.vader import SentimentIntensityAnalyzer # To calculate sentiment scores
# Download the VADER lexicon
nltk.download('vader_lexicon') # to compute sentiment scores including wprds, phrases and emojis

# Initialize the Sentiment Analyzer
sia = SentimentIntensityAnalyzer()

# Apply VADER sentiment analysis
df['sentiment_scores'] = df['cleaned_text'].apply(lambda x: sia.polarity_scores(x))
df['compound'] = df['sentiment_scores'].apply(lambda score_dict: score_dict['compound'])

# Label Sentiments
def label_sentiment(compound_score):
    if compound_score >= 0.34:
        return 'Positive'
    elif compound_score <= -0.05:
        return 'Negative'
    else:
        return 'Neutral'

df['sentiment'] = df['compound'].apply(label_sentiment)

# Save the Labeled Data
df.to_csv('labeled_tweets.csv', index=False)

"""The labeled data is save to a new csv file, labeled_tweets.csv.

# Sentiment Visualisation
"""

# Visualize the distribution of Sentiment (Positive, Negative & Neutral)
import matplotlib.pyplot as plt
sentiment_counts = df['sentiment'].value_counts()
sentiment_counts.plot(kind='bar', color=['green', 'yellow', 'red'])
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Number of Tweets')
plt.show()

print(sentiment_counts)

# Count the sentiment distribution for each smartphone
sentiment_counts = df.groupby(['smartphone', 'sentiment']).size().reset_index(name='count')

# Pivot the data for easier visualization
pivot_data = sentiment_counts.pivot(index='smartphone', columns='sentiment', values='count').fillna(0)

# Define a custom color palette: Negative = Red, Positive = Green, Neutral = Yellow
custom_palette = {
    'Negative': 'red',
    'Positive': 'green',
    'Neutral': 'yellow'
}

# Plot the sentiment distribution with the custom color palette
plt.figure(figsize=(10, 6))
sns.barplot(x='smartphone', y='count', hue='sentiment', data=sentiment_counts, palette=custom_palette)


# Add labels and title
plt.title('Sentiment Distribution for Each Smartphone', fontsize=16)
plt.xlabel('Smartphone', fontsize=12)
plt.ylabel('Tweet Count', fontsize=12)
plt.xticks(rotation=45)
plt.legend(title='Sentiment')
plt.tight_layout()

# Show the plot
plt.show()

# Import necessary libraries
from collections import defaultdict


# Features to analyze
features = ['design', 'display', 'camera', 'performance', 'battery', 'software', 'price']

# Function to extract adjectives using POS tagging
def extract_adjectives(text, feature):
    """
    Extract adjectives related to a specific feature in the text.
    """
    words = nltk.word_tokenize(text)
    pos_tags = nltk.pos_tag(words)
    adjectives = []
    for idx, (word, tag) in enumerate(pos_tags):
        if word == feature:
            # Look for adjectives in a window around the feature
            window = pos_tags[max(0, idx - 2):idx + 3]
            adjectives.extend([w for w, t in window if t == 'JJ'])
    return adjectives

# Function to analyze sentiment for specific features
def analyze_sentiment_with_adjectives(row, features):
    """
    Analyze sentiment for given features in the text, using the pre-labeled compound score
    and extracting adjectives with POS tagging.
    """
    sentiment_results = defaultdict(lambda: {'positive': 0, 'negative': 0, 'neutral': 0, 'adjectives': []})
    for feature in features:
        if feature in row['cleaned_text']:
            # Use the pre-calculated compound score
            compound = row['compound']
            if compound > 0.34:
                sentiment_results[feature]['positive'] += 1
            elif compound < -0.05:
                sentiment_results[feature]['negative'] += 1
            else:
                sentiment_results[feature]['neutral'] += 1
            # Extract adjectives using POS tagging
            adjectives = extract_adjectives(row['cleaned_text'], feature)
            sentiment_results[feature]['adjectives'].extend(adjectives)
    return sentiment_results

# Apply the updated function to the DataFrame
df['sentiment_results'] = df.apply(lambda row: analyze_sentiment_with_adjectives(row, features), axis=1)

# Summarize results by smartphone model
feature_sentiment_summary = defaultdict(lambda: defaultdict(dict))

for model, group in df.groupby('smartphone'):
    model_results = defaultdict(lambda: {'positive': 0, 'negative': 0, 'neutral': 0, 'adjectives': []})
    for sentiment in group['sentiment_results']:
        for feature, values in sentiment.items():
            for key in ['positive', 'negative', 'neutral']:
                model_results[feature][key] += values[key]
            model_results[feature]['adjectives'].extend(values['adjectives'])
    feature_sentiment_summary[model] = model_results

# Display summarized sentiment results
for model, results in feature_sentiment_summary.items():
    print(f"Smartphone Model: {model}")
    for feature, sentiment in results.items():
        print(f"Feature: {feature}, Positive: {sentiment['positive']}, Negative: {sentiment['negative']}, Neutral: {sentiment['neutral']}")
        print(f"Adjectives: {set(sentiment['adjectives'])}")

# Aggregate data for visualization
sentiment_counts = []

for model, results in feature_sentiment_summary.items():
    for feature, sentiment in results.items():
        sentiment_counts.append({
            'smartphone': model,
            'feature': feature,
            'positive': sentiment['positive'],
            'negative': sentiment['negative'],
            'neutral': sentiment['neutral']
        })

sentiment_df = pd.DataFrame(sentiment_counts)

# Define a custom color palette with red for negative, green for positive, yellow for neutral
custom_palette = {
    'negative': 'red',
    'positive': 'green',
    'neutral': 'yellow'
}

# Melt the dataframe for better compatibility with seaborn
melted_sentiment_df = sentiment_df.melt(id_vars=['smartphone', 'feature'],
                                        var_name='sentiment',
                                        value_name='count')

# Enhanced bar plot for sentiment with smartphone differentiation using the custom palette
plt.figure(figsize=(14, 10))
sns.barplot(data=melted_sentiment_df, x='feature', y='count', hue='sentiment', palette=custom_palette, errorbar=None)

# Adding title and labels
plt.title('Sentiment Counts for Features Across Smartphone Models', fontsize=16)
plt.xlabel('Feature', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.xticks(rotation=45, fontsize=12)
plt.legend(title='Sentiment', fontsize=12, title_fontsize=14)
plt.tight_layout()

# Show the plot
plt.show()

# Transform data for FacetGrid
melted_df = sentiment_df.melt(id_vars=['smartphone', 'feature'],
                              var_name='sentiment', value_name='count')
# Set color palette for consistency
sentiment_colors = {"positive": "green", "negative": "red", "neutral": "yellow"}
# Create FacetGrid
g = sns.FacetGrid(data=melted_df, col='smartphone', col_wrap=2, height=5, sharey=True)
g.map(sns.barplot, 'feature', 'count', 'sentiment', palette=sentiment_colors, ci=None, order=sentiment_df['feature'].unique())
g.set_titles("{col_name}")
g.set_axis_labels("Feature", "Count")
g.set_xticklabels(rotation=45)
g.add_legend(title="Sentiment")

plt.subplots_adjust(top=0.9)
g.fig.suptitle('Sentiment Counts for Features by Smartphone Models', fontsize=16)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Aggregate data for visualization (Separating Samsung and iPhone Models)
sentiment_counts = []

for model, results in feature_sentiment_summary.items():
    for feature, sentiment in results.items():
        sentiment_counts.append({
            'smartphone': model,
            'feature': feature,
            'positive': sentiment['positive'],
            'negative': sentiment['negative'],
            'neutral': sentiment['neutral']
        })

# Convert to DataFrame
sentiment_df = pd.DataFrame(sentiment_counts)

# Split data for Samsung and iPhone models
samsung_models = sentiment_df[sentiment_df['smartphone'].str.contains('Samsung', case=False)]
iphone_models = sentiment_df[sentiment_df['smartphone'].str.contains('iPhone', case=False)]

# Set color palette for consistency
sentiment_colors = {"positive": "green", "negative": "red", "neutral": "yellow"}

# Create the first bar plot for iPhone (Error bars disabled)
plt.figure(figsize=(14, 10))
sns.barplot(data=iphone_models.melt(id_vars=['smartphone', 'feature'],
                                    var_name='sentiment',
                                    value_name='count'),
            x='feature', y='count', hue='sentiment', palette=sentiment_colors, errorbar=None)
plt.title('Feature-Based Sentiment Analysis for iPhone Models', fontsize=18)
plt.xlabel('Feature', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.xticks(rotation=45)
plt.tight_layout()
plt.legend(title='Sentiment')
plt.show()

# Create the second bar plot for Samsung (Error bars disabled)
plt.figure(figsize=(14, 10))
sns.barplot(data=samsung_models.melt(id_vars=['smartphone', 'feature'],
                                     var_name='sentiment',
                                     value_name='count'),
            x='feature', y='count', hue='sentiment', palette=sentiment_colors, errorbar=None)
plt.title('Feature-Based Sentiment Analysis for Samsung Models', fontsize=18)
plt.xlabel('Feature', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.xticks(rotation=45)
plt.tight_layout()
plt.legend(title='Sentiment')
plt.show()

from wordcloud import WordCloud

# Generate word clouds for positive, negative, and neutral adjectives
for sentiment_type in ['positive', 'negative', 'neutral']:
    adjectives = []
    for model_results in feature_sentiment_summary.values():
        for feature_sentiment in model_results.values():
            adjectives.extend(feature_sentiment['adjectives'])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(adjectives))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'{sentiment_type.capitalize()} Adjectives Word Cloud')
    plt.show()

# Table summarizing sentiments for each feature
sentiment_table = sentiment_df.pivot_table(index='smartphone', columns='feature', values=['positive', 'negative', 'neutral'], aggfunc='sum')
print(sentiment_table)

# Save the Labeled Data
df.to_csv('labeled_tweets2.csv', index=False)
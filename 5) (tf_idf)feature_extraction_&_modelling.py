# -*- coding: utf-8 -*-
"""(TF-IDF)Feature Extraction & Modelling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vIpnwWR7RpnX0QOiyS7a8G2VcexuOqmk

# FEATURE EXTRACTION & DATA MODELLING

## Dataset Loading and Checking
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv(r'Dataset\\labeled_tweets.csv')

# Mapping sentiment to numerical values
sentiment_mapping = {'Positive': 1, 'Neutral': 0, 'Negative': -1}
data['sentiment_numerical'] = data['sentiment'].map(sentiment_mapping)

# Select the text and sentiment columns
text_column = 'cleaned_text'
label_column = 'sentiment_numerical'
texts = data[text_column].astype(str)
labels = data[label_column]

# Display basic stats about the dataset
print("\nDataset Info:")
print(data.info())
print(labels)

print("\nNumber of Samples:", len(data))
print("Number of Positive Labels:", sum(labels == 1 ))
print("Number of Negative Labels:", sum(labels == -1 ))
print("Number of Neutral Labels:", sum(labels == 0 ))

# Check for missing values
print("\nMissing Values:")
print(data.isnull().sum())

# Drop rows with missing values
data = data.dropna(subset=[text_column, label_column])

# Remove duplicates
data = data.drop_duplicates()

print("\nCleaned Dataset Size:", len(data))

"""## Count the sentiment distribution for each smartphone"""

# Count the sentiment distribution for each smartphone
sentiment_counts = data.groupby(['smartphone', 'sentiment']).size().reset_index(name='count')

# Pivot the data for easier visualization
pivot_data = sentiment_counts.pivot(index='smartphone', columns='sentiment', values='count').fillna(0)

# Display the data
print("\nSentiment Counts for Each Smartphone:")
print(pivot_data)

# Plot the sentiment distribution
plt.figure(figsize=(10, 6))
sns.barplot(x='smartphone', y='count', hue='sentiment', data=sentiment_counts, palette='viridis')

# Add labels and title
plt.title('Sentiment Distribution for Each Smartphone', fontsize=16)
plt.xlabel('Smartphone', fontsize=12)
plt.ylabel('Tweet Count', fontsize=12)
plt.xticks(rotation=45)
plt.legend(title='Sentiment')
plt.tight_layout()

# Show the plot
plt.show()

"""# FEATURE EXTRACTION

Feature extraction is done using TF-IDF representation to convert text into numerical data.

## TF-IDF (Term Frequency-Inverse Document Frequency):

- Measures the importance of words in a document relative to the dataset.
- TfidfVectorizer creates the TF-IDF matrix.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Unigrams and bigrams
tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # (1, 2) means unigrams and bigrams
tfidf_matrix = tfidf_vectorizer.fit_transform(texts)

print("TF-IDF with Unigrams and Bigrams Shape:", tfidf_matrix.shape)

# Apply SMOTE to balance the classes in the training set
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(tfidf_matrix, labels)

"""## Train-Test Split"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_train_resampled, y_train_resampled, test_size=0.2, random_state=42
)

print("Training Set Shape:", X_train.shape)
print("Testing Set Shape:", X_test.shape)

"""Unigrams (single words) and bigrams (two-word phrases) are considered using ngram_range=(1, 2)"""

# Check the class distribution after resampling
from collections import Counter

resampled_distribution = Counter(y_train)
print("Class distribution after resampling:")
print(resampled_distribution)

"""# MODEL TRAINING & EVALUATION

## Naive Bayes
"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score

# Train the Naive Bayes model
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

# Make predictions
y_pred_nb = nb_model.predict(X_test)

# Evaluate the model
print("Naive Bayes Classification Report:")
print(classification_report(y_test, y_pred_nb))
print("Naive Bayes Accuracy: {:.2f}%".format(accuracy_score(y_test, y_pred_nb) * 100))

"""## Support Vector Machine"""

from sklearn.svm import LinearSVC

# Train the SVM model
svm_model = LinearSVC(random_state=42)
svm_model.fit(X_train, y_train)

# Make predictions
y_pred_svm = svm_model.predict(X_test)

# Evaluate the model
print("SVM Classification Report:")
print(classification_report(y_test, y_pred_svm))
print("SVM Accuracy: {:.2f}%".format(accuracy_score(y_test, y_pred_svm) * 100))

"""## Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

# Train the Decision Tree model
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)

# Make predictions
y_pred_dt = dt_model.predict(X_test)

# Evaluate the model
print("Decision Tree Classification Report:")
print(classification_report(y_test, y_pred_dt))
print("Decision Tree Accuracy: {:.2f}%".format(accuracy_score(y_test, y_pred_dt) * 100))

import pandas as pd

# Collect accuracies
model_results = {
    "Model": ["Naive Bayes", "SVM", "Decision Tree"],
    "Accuracy": [
        accuracy_score(y_test, y_pred_nb),
        accuracy_score(y_test, y_pred_svm),
        accuracy_score(y_test, y_pred_dt),
    ],
}

# Create a DataFrame
results_df = pd.DataFrame(model_results)
print(results_df)

"""Model  Accuracy
0    Naive Bayes  0.609890
1            SVM  0.799451
2  Decision Tree  0.671978

Model  Accuracy
0    Naive Bayes  0.489831
1            SVM  0.737853
2  Decision Tree  0.632768
"""

from sklearn.svm import LinearSVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, accuracy_score

# Define the parameter grid for LinearSVC
param_grid = {
    'C': [0.01, 0.1, 1, 10],  # Regularization strength
    'max_iter': [1000, 5000]  # Maximum number of iterations
}

# Initialize the SVM model
svm_model = LinearSVC(random_state=42)

# Initialize GridSearchCV
grid_search_svm = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)

# Fit GridSearchCV
grid_search_svm.fit(X_train, y_train)

# Display the best parameters and accuracy
print("Best Parameters for SVM:", grid_search_svm.best_params_)
print("Best Cross-Validated Accuracy: {:.2f}%".format(grid_search_svm.best_score_ * 100))

# Evaluate the best model on the test set
best_svm_model = grid_search_svm.best_estimator_
y_pred_best_svm = best_svm_model.predict(X_test)

print("\nTest Set Evaluation with Tuned SVM:")
print(classification_report(y_test, y_pred_best_svm))
print("Test Set Accuracy: {:.2f}%".format(accuracy_score(y_test, y_pred_best_svm) * 100))

"""Test Set Evaluation with Tuned SVM:
              precision    recall  f1-score   support

    Negative       0.72      0.65      0.68       444
     Neutral       0.71      0.65      0.68       523
    Positive       0.76      0.84      0.80       803

    accuracy                           0.74      1770
   macro avg       0.73      0.71      0.72      1770
weighted avg       0.74      0.74      0.74      1770
"""

# Import necessary libraries for preprocessing
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk import pos_tag
from contractions import fix
# Ensure stopwords and lemmatizer setup
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('wordnet')
nltk.download('punkt_tab')
nltk.download('punkt')
# List of custom stopwords to retain, including "no" and "not"
default_stopwords = set(stopwords.words('english'))
custom_stopwords = default_stopwords - {'no', 'not', 'nor', 'none', 'never', 'neither', 'without', 'against',
                                        'but', 'however', 'though', 'although',
                                        'because', 'since', 'due to', 'with'}


# Words that should not be lemmatized
non_lemmatizable_words = {'iphone16', 'iphone16plus', 'iphone16pro', 'iphone16promax', "ios", 'iphone15', 'iphone15plus', 'iphone15pro', 'iphone15promax',
                          'samsunggalaxy23', 'samsunggalaxys23plus', 'samsunggalaxys23ultra', 'samsunggalaxy24', 'samsunggalaxys24plus', 'samsunggalaxys24ultra',
                          'ios17', 'ios18', 'dynamic island', 'a17bionic', 'a18chip', 'usb-c', 'lightning port', 'pro motion', 'ceramic shield',
                          'snapdragon', 'exynos', '120hz', 'amozed', 'one ui',
                          '5g', 'refresh rate', 'fast charging', 'screen size'
                          }

# Function to map POS tags to WordNet tags
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # Default to noun

# Function for text preprocessing
def preprocess_text(text):
    # Remove mentions
    text = re.sub(r'@\w+', '', text)
    # Expand contractions (assuming `fix()` function is defined elsewhere)
    text = fix(text)
    # Remove URLs
    text = re.sub(r'http\S+|www\.\S+', '', text)
    # Remove special characters and punctuation, but keep numbers and dots
    text = re.sub(r'[^a-zA-Z0-9.\s]', '', text)
    # Convert to lowercase
    text = text.lower()
    # Remove redundant whitespaces
    text = re.sub(r'\s+', ' ', text).strip()
    # Tokenize and POS tag
    tokens = word_tokenize(text)
    pos_tags = pos_tag(tokens)
    lemmatizer = WordNetLemmatizer()
    processed_tokens = [
        lemmatizer.lemmatize(word, get_wordnet_pos(tag))
        if word.isalpha() and word not in custom_stopwords else word
        for word, tag in pos_tags
    ]

    return ' '.join(processed_tokens)

# Function to predict sentiment for new tweets with preprocessing
def predict_tweet_sentiment(tweet_text):
    cleaned_text = preprocess_text(tweet_text)
    tweet_vectorized = tfidf_vectorizer.transform([cleaned_text])
    prediction = best_svm_model.predict(tweet_vectorized)
    sentiment_mapping_reverse = {1: 'Positive', 0: 'Neutral', -1: 'Negative'}
    return sentiment_mapping_reverse[prediction[0]]

# Example predictions with preprocessing
new_tweet_1 = "iphone has no security and very ugly design"
new_tweet_2 = "iphone is affordable to buy"
new_tweet_3 = "iphone is very expensive"

predicted_sentiment_1 = predict_tweet_sentiment(new_tweet_1)
predicted_sentiment_2 = predict_tweet_sentiment(new_tweet_2)
predicted_sentiment_3 = predict_tweet_sentiment(new_tweet_3)

print("\nNew Tweet Sentiment Predictions:")
print(f"Tweet: {preprocess_text(new_tweet_1)}")
print(f"Predicted Sentiment: {predicted_sentiment_1}")
print(f"\nTweet: {preprocess_text(new_tweet_2)}")
print(f"Predicted Sentiment: {predicted_sentiment_2}")
print(f"\nTweet: {preprocess_text(new_tweet_3)}")
print(f"Predicted Sentiment: {predicted_sentiment_3}")

import joblib
# Save the model and vectorizer
joblib.dump(best_svm_model, 'svm_model.pkl')
joblib.dump(tfidf_vectorizer, 'vectorizer.pkl')
print("Model and vectorizer saved successfully.")

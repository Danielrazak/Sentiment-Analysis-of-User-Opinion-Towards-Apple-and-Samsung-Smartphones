{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BAG OF WORD FEATURE EXTRACTION\n",
        "\n"
      ],
      "metadata": {
        "id": "qkoO8o4FcyGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries and Loading Data"
      ],
      "metadata": {
        "id": "baVSlDoCcjaK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBcJQsbqiEa-",
        "outputId": "34323c0f-acbb-4afa-a3df-97876a585081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8850 entries, 0 to 8849\n",
            "Data columns (total 14 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   conversation_id_str  8850 non-null   float64\n",
            " 1   favorite_count       8850 non-null   int64  \n",
            " 2   full_text            8850 non-null   object \n",
            " 3   lang                 8850 non-null   object \n",
            " 4   reply_count          8850 non-null   int64  \n",
            " 5   retweet_count        8850 non-null   int64  \n",
            " 6   smartphone           8850 non-null   object \n",
            " 7   Month                8850 non-null   int64  \n",
            " 8   Year                 8850 non-null   int64  \n",
            " 9   cleaned_text         8850 non-null   object \n",
            " 10  sentiment_scores     8850 non-null   object \n",
            " 11  compound             8850 non-null   float64\n",
            " 12  sentiment            8850 non-null   object \n",
            " 13  sentiment_numerical  8850 non-null   int64  \n",
            "dtypes: float64(2), int64(6), object(6)\n",
            "memory usage: 968.1+ KB\n",
            "None\n",
            "0      -1\n",
            "1      -1\n",
            "2      -1\n",
            "3      -1\n",
            "4      -1\n",
            "       ..\n",
            "8845    1\n",
            "8846    1\n",
            "8847    1\n",
            "8848    1\n",
            "8849    1\n",
            "Name: sentiment_numerical, Length: 8850, dtype: int64\n",
            "\n",
            "Number of Samples: 8850\n",
            "Number of Positive Labels: 4047\n",
            "Number of Negative Labels: 2194\n",
            "Number of Neutral Labels: 2609\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/SEM 5/DS PROJ/DSP/labeled_tweets.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Mapping sentiment to numerical values\n",
        "sentiment_mapping = {'Positive': 1, 'Neutral': 0, 'Negative': -1}\n",
        "data['sentiment_numerical'] = data['sentiment'].map(sentiment_mapping)\n",
        "\n",
        "# Select the text and sentiment columns\n",
        "text_column = 'cleaned_text'\n",
        "label_column = 'sentiment_numerical'\n",
        "texts = data[text_column].astype(str)\n",
        "labels = data[label_column]\n",
        "\n",
        "# Display basic stats about the dataset\n",
        "print(\"\\nDataset Info:\")\n",
        "print(data.info())\n",
        "print(labels)\n",
        "\n",
        "print(\"\\nNumber of Samples:\", len(data))\n",
        "print(\"Number of Positive Labels:\", sum(labels == 1 ))\n",
        "print(\"Number of Negative Labels:\", sum(labels == -1 ))\n",
        "print(\"Number of Neutral Labels:\", sum(labels == 0 ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature extraction is done using Bag of Words (BOW) representation to convert text into numerical data.\n",
        "\n",
        "## Bag of Words (BOW):\n",
        "\n",
        "- Converts text into a matrix of token counts.\n",
        "- Each row represents a document, and each column corresponds to a token.\n",
        "- The CountVectorizer is used to create the BOW matrix, and the shape of the matrix (rows, features) is printed.\n"
      ],
      "metadata": {
        "id": "3frBtew-c1pL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrSOtqBPm1A_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "117e52e7-03a4-4c00-d933-eb49496fe15b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOW with Unigrams and Bigrams Shape: (8850, 103583)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Unigrams and bigrams\n",
        "bow_vectorizer = CountVectorizer(ngram_range=(1, 2))  # (1, 2) means unigrams and bigrams\n",
        "bow_matrix = bow_vectorizer.fit_transform(texts)\n",
        "\n",
        "print(\"BOW with Unigrams and Bigrams Shape:\", bow_matrix.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Imbalanced Data with SMOTE"
      ],
      "metadata": {
        "id": "A1sme2xBPPMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Number of Samples: 8850\n",
        "Number of Positive Labels: 4047\n",
        "Number of Negative Labels: 2194\n",
        "Number of Neutral Labels: 2609\n",
        "```\n",
        "Number of positive labels higher than negative and neutral label, thus it might lead to biased model performance, where the model may favor the majority class (positive) and underperform on the minority classes (negative & neutral)\n",
        "\n"
      ],
      "metadata": {
        "id": "nQZhoAtzPSEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply SMOTE to balance the classes in the training set\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(bow_matrix, labels)"
      ],
      "metadata": {
        "id": "pbV-TqK-sNcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baadc109-97fd-424c-b53a-0e3fb7e10928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nNumber of Samples:\", len(data))\n",
        "print(\"Number of Positive Labels:\", sum(y_train_resampled == 1 ))\n",
        "print(\"Number of Negative Labels:\", sum(y_train_resampled == -1 ))\n",
        "print(\"Number of Neutral Labels:\", sum(y_train_resampled == 0 ))"
      ],
      "metadata": {
        "id": "2OA1UBT_3q_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350d258a-cb82-4080-9dd1-d942fe4dd573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of Samples: 8850\n",
            "Number of Positive Labels: 4047\n",
            "Number of Negative Labels: 4047\n",
            "Number of Neutral Labels: 4047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Test Split"
      ],
      "metadata": {
        "id": "biEivi_3dPy-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfikPltTlde2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ce2f51-06b3-4b79-a415-5bb2d6ec1c34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Shape: (9712, 103583)\n",
            "Testing Set Shape: (2429, 103583)\n",
            "Class distribution after resampling:\n",
            "Counter({1: 3256, -1: 3246, 0: 3210})\n"
          ]
        }
      ],
      "source": [
        "# To split dataset to train and tes\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Check the class distribution after resampling\n",
        "from collections import Counter\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_train_resampled, y_train_resampled, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training Set Shape:\", X_train.shape)\n",
        "print(\"Testing Set Shape:\", X_test.shape)\n",
        "\n",
        "resampled_distribution = Counter(y_train)\n",
        "print(\"Class distribution after resampling:\")\n",
        "print(resampled_distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Number of Samples: 8850\n",
        "Number of Positive Labels: 4047\n",
        "Number of Negative Labels: 2194\n",
        "Number of Neutral Labels: 2609\n",
        "```"
      ],
      "metadata": {
        "id": "jk2onNG83gFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL TRAINING & EVALUATION"
      ],
      "metadata": {
        "id": "Qm4HKtYydO7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "0Bb_ljeCRv5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes"
      ],
      "metadata": {
        "id": "Zo-c6b1CdYfq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-awxg7xnazt"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Train the Naive Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_nb = nb_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machine"
      ],
      "metadata": {
        "id": "TpiHdiNideyB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MwWockJneld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd7efbfa-7b0e-46e0-9e69-faf32cf9e450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Train the SVM model\n",
        "svm_model = LinearSVC(random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and decision scores\n",
        "y_pred_svm = svm_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "D13xEcGtdmoa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozFm77sOnhKp"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Train the Decision Tree model\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and probabilities\n",
        "y_pred_dt = dt_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGMw1iwQoSi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cee34a64-a2d7-4fdc-8981-8a61e1f736c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Naive Bayes Evaluation:\n",
            "Accuracy: 0.57\n",
            "F1-Score: 0.55\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.57      0.68      0.62       801\n",
            "           0       0.79      0.28      0.41       837\n",
            "           1       0.52      0.77      0.62       791\n",
            "\n",
            "    accuracy                           0.57      2429\n",
            "   macro avg       0.63      0.58      0.55      2429\n",
            "weighted avg       0.63      0.57      0.55      2429\n",
            "\n",
            "\n",
            "Support Vector Machine Evaluation:\n",
            "Accuracy: 0.72\n",
            "F1-Score: 0.72\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.68      0.76      0.72       801\n",
            "           0       0.68      0.69      0.69       837\n",
            "           1       0.82      0.71      0.76       791\n",
            "\n",
            "    accuracy                           0.72      2429\n",
            "   macro avg       0.73      0.72      0.72      2429\n",
            "weighted avg       0.73      0.72      0.72      2429\n",
            "\n",
            "\n",
            "Decision Tree Evaluation:\n",
            "Accuracy: 0.65\n",
            "F1-Score: 0.65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.59      0.69      0.64       801\n",
            "           0       0.67      0.61      0.63       837\n",
            "           1       0.70      0.65      0.67       791\n",
            "\n",
            "    accuracy                           0.65      2429\n",
            "   macro avg       0.65      0.65      0.65      2429\n",
            "weighted avg       0.65      0.65      0.65      2429\n",
            "\n",
            "\n",
            "Summary of Model Performance:\n",
            "           Model  Accuracy  F1-Score\n",
            "0    Naive Bayes  0.573075  0.549985\n",
            "1            SVM  0.720461  0.721404\n",
            "2  Decision Tree  0.647592  0.647933\n"
          ]
        }
      ],
      "source": [
        "# Define a function to calculate Accuracy and F1-Score\n",
        "def evaluate_model(model_name, y_test, y_pred):\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')  # Consistent usage of weighted F1-Score\n",
        "    print(f\"\\n{model_name} Evaluation:\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    return accuracy, f1\n",
        "\n",
        "\n",
        "# Evaluate Models\n",
        "acc_nb, f1_nb = evaluate_model(\"Naive Bayes\", y_test, y_pred_nb)\n",
        "acc_svm, f1_svm = evaluate_model(\"Support Vector Machine\", y_test, y_pred_svm)\n",
        "acc_dt, f1_dt = evaluate_model(\"Decision Tree\", y_test, y_pred_dt)\n",
        "\n",
        "model_results = pd.DataFrame({\n",
        "    \"Model\": [\"Naive Bayes\", \"SVM\", \"Decision Tree\"],\n",
        "    \"Accuracy\": [acc_nb, acc_svm, acc_dt],\n",
        "    \"F1-Score\": [f1_nb, f1_svm, f1_dt]\n",
        "})\n",
        "\n",
        "print(\"\\nSummary of Model Performance:\")\n",
        "print(model_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation and Hyperparameter Tuning on Best Model (SVM)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SxdL4c0mIKk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for LinearSVC\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
        "    'max_iter': [1000, 5000]  # Maximum number of iterations\n",
        "}\n",
        "\n",
        "# Initialize the SVM model\n",
        "svm_model = LinearSVC(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV with F1-score as part of scoring metrics\n",
        "grid_search_svm = GridSearchCV(\n",
        "    estimator=svm_model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    scoring='f1_weighted',  # Scoring changed to F1-Score for better optimization\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search_svm.fit(X_train, y_train)\n",
        "\n",
        "# Display the best parameters and best F1-Score\n",
        "print(\"Best Parameters for SVM:\", grid_search_svm.best_params_)\n",
        "print(\"Best Cross-Validated F1-Score: {:.2f}%\".format(grid_search_svm.best_score_ * 100))\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_svm_model = grid_search_svm.best_estimator_\n",
        "y_pred_best_svm = best_svm_model.predict(X_test)\n",
        "\n",
        "# Final Model Evaluation\n",
        "print(\"\\nTest Set Evaluation with Tuned SVM:\")\n",
        "print(classification_report(y_test, y_pred_best_svm))\n",
        "test_accuracy = accuracy_score(y_test, y_pred_best_svm)\n",
        "test_f1_score = f1_score(y_test, y_pred_best_svm, average='weighted')\n",
        "\n",
        "# Print Accuracy and F1-Score\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.2f}\")\n",
        "print(f\"Test Set F1-Score: {test_f1_score:.2f}\")"
      ],
      "metadata": {
        "id": "EVHKHIeFlucS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2594115-dcc8-4faa-c5df-005e3c43fde2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters for SVM: {'C': 0.1, 'max_iter': 1000}\n",
            "Best Cross-Validated F1-Score: 71.91%\n",
            "\n",
            "Test Set Evaluation with Tuned SVM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.71      0.67      0.69       801\n",
            "           0       0.66      0.76      0.71       837\n",
            "           1       0.81      0.74      0.77       791\n",
            "\n",
            "    accuracy                           0.72      2429\n",
            "   macro avg       0.73      0.72      0.72      2429\n",
            "weighted avg       0.73      0.72      0.72      2429\n",
            "\n",
            "Test Set Accuracy: 0.72\n",
            "Test Set F1-Score: 0.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the sentiment prediction using Best Model and TFIDF Vectorizer"
      ],
      "metadata": {
        "id": "0o-g9OtQQY4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "import re # Regular, to peform text cleaning (remove urls, special characters and extra spaces)\n",
        "import nltk # NLTK is used for NLP process mainly in text preprocessing (Tokenization, stopword removal, lemmatization, POS taagging)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "from contractions import fix #To expand contracted words like \"can't\" to \"cannot\".\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt_tab') # for tokenizing sentences or words\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet') # For Wordnet, used in lemmatization to access synonyms, definations, and for lemmatization\n",
        "nltk.download('omw-1.4')  # WordNet multilingual extensions, to support multilingual\n",
        "nltk.download('averaged_perceptron_tagger_eng') # For part-of-speech tagging\n",
        "nltk.download('stopwords') # For stopword removal during text preprocessing\n",
        "\n",
        "# List of custom stopwords to retain from removing, including \"no\" and \"not\"\n",
        "default_stopwords = set(stopwords.words('english'))\n",
        "custom_stopwords = default_stopwords - {'no', 'not', 'nor', 'none', 'never', 'neither', 'without', 'against',\n",
        "                                        'but', 'however', 'though', 'although',\n",
        "                                        'because', 'since', 'due to', 'with'}\n",
        "\n",
        "\n",
        "# Words that should not be lemmatized\n",
        "non_lemmatizable_words = {'iphone16', 'iphone16plus', 'iphone16pro', 'iphone16promax', \"ios\", 'iphone15', 'iphone15plus', 'iphone15pro', 'iphone15promax',\n",
        "                          'samsunggalaxy23', 'samsunggalaxys23plus', 'samsunggalaxys23ultra', 'samsunggalaxy24', 'samsunggalaxys24plus', 'samsunggalaxys24ultra',\n",
        "                          'ios17', 'ios18', 'dynamic island', 'a17bionic', 'a18chip', 'usb-c', 'lightning port', 'pro motion', 'ceramic shield',\n",
        "                          'snapdragon', 'exynos', '120hz', 'amozed', 'one ui',\n",
        "                          '5g', 'refresh rate', 'fast charging', 'screen size'\n",
        "                          }\n",
        "\n",
        "# Function to map POS tags to WordNet tags\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun\n",
        "\n",
        "# Function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Remove mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Expand contractions (assuming `fix()` function is defined elsewhere)\n",
        "    text = fix(text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "    # Remove special characters and punctuation, but keep numbers and dots\n",
        "    text = re.sub(r'[^a-zA-Z0-9.\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove redundant whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize and POS tag\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Enhanced: Edge case handling for empty text\n",
        "    if not tokens:\n",
        "        return ''  # Return an empty string if no valid tokens remain\n",
        "\n",
        "    # Processing tokens with enhanced handling\n",
        "    processed_tokens = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
        "        if word.isalpha() and word not in custom_stopwords and word not in non_lemmatizable_words\n",
        "        else word\n",
        "        for word, tag in pos_tags\n",
        "    ]\n",
        "\n",
        "    return ' '.join(processed_tokens)\n"
      ],
      "metadata": {
        "id": "iwFZwoWgEfsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4942d51d-4ac1-4dde-824e-f321cab19bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict sentiment for new tweets with preprocessing\n",
        "def predict_tweet_sentiment(tweet_text):\n",
        "    cleaned_text = preprocess_text(tweet_text)\n",
        "    tweet_vectorized = bow_vectorizer.transform([cleaned_text])\n",
        "    prediction = best_svm_model.predict(tweet_vectorized)\n",
        "    sentiment_mapping_reverse = {1: 'Positive', 0: 'Neutral', -1: 'Negative'}\n",
        "    return sentiment_mapping_reverse[prediction[0]]\n",
        "\n",
        "# Example predictions with preprocessing\n",
        "new_tweet_1 = \"iphone has no security and very ugly design\"\n",
        "new_tweet_2 = \"iphone15 is affordable\"\n",
        "new_tweet_3 = \"iphone is expensive\"\n",
        "\n",
        "predicted_sentiment_1 = predict_tweet_sentiment(new_tweet_1)\n",
        "predicted_sentiment_2 = predict_tweet_sentiment(new_tweet_2)\n",
        "predicted_sentiment_3 = predict_tweet_sentiment(new_tweet_3)\n",
        "\n",
        "print(\"\\nNew Tweet Sentiment Predictions:\")\n",
        "print(f\"Tweet: {preprocess_text(new_tweet_1)}\")\n",
        "print(f\"Predicted Sentiment: {predicted_sentiment_1}\")\n",
        "print(f\"\\nTweet: {preprocess_text(new_tweet_2)}\")\n",
        "print(f\"Predicted Sentiment: {predicted_sentiment_2}\")\n",
        "print(f\"\\nTweet: {preprocess_text(new_tweet_3)}\")\n",
        "print(f\"Predicted Sentiment: {predicted_sentiment_3}\")"
      ],
      "metadata": {
        "id": "TCrcOiSQGra0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74b28d95-f5cc-4d70-acfc-542c131982f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "New Tweet Sentiment Predictions:\n",
            "Tweet: iphone has no security and very ugly design\n",
            "Predicted Sentiment: Negative\n",
            "\n",
            "Tweet: iphone15 is affordable\n",
            "Predicted Sentiment: Negative\n",
            "\n",
            "Tweet: iphone is expensive\n",
            "Predicted Sentiment: Neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Saving and Loading"
      ],
      "metadata": {
        "id": "Z05sjCZuQ8a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "# Save the model and vectorizer\n",
        "joblib.dump(best_svm_model, 'svm_model.pkl')\n",
        "joblib.dump(bow_vectorizer, 'vectorizer.pkl')\n",
        "print(\"Model and vectorizer saved successfully.\")"
      ],
      "metadata": {
        "id": "DVeIt7iuRzXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a5c9897-f304-431d-ca80-a8b32803bc77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and vectorizer saved successfully.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
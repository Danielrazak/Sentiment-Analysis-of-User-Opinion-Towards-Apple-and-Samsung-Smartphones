# -*- coding: utf-8 -*-
"""(TF-IDF)Feature Extraction & Modelling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vIpnwWR7RpnX0QOiyS7a8G2VcexuOqmk

# TF-IDF FEATURE EXTRACTION

## Importing Libraries and Loading Data
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv(r'labeled_tweets.csv')

# Mapping sentiment to numerical values
sentiment_mapping = {'Positive': 1, 'Neutral': 0, 'Negative': -1}
data['sentiment_numerical'] = data['sentiment'].map(sentiment_mapping)

# Select the text and sentiment columns
text_column = 'cleaned_text'
label_column = 'sentiment_numerical'
texts = data[text_column].astype(str)
labels = data[label_column]

# Display basic stats about the dataset
print("\nDataset Info:")
print(data.info())
print(labels)

print("\nNumber of Samples:", len(data))
print("Number of Positive Labels:", sum(labels == 1 ))
print("Number of Negative Labels:", sum(labels == -1 ))
print("Number of Neutral Labels:", sum(labels == 0 ))

"""Feature extraction is done using TF-IDF representation to convert text into numerical data.

TF-IDF (Term Frequency-Inverse Document Frequency):

- Measures the importance of words in a document relative to the dataset.
- TfidfVectorizer creates the TF-IDF matrix.




"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Unigrams and bigrams
tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # (1, 2) means unigrams and bigrams
tfidf_matrix = tfidf_vectorizer.fit_transform(texts)

print("TF-IDF with Unigrams and Bigrams Shape:", tfidf_matrix.shape)

"""## Handling Imbalanced Data with SMOTE

```
Number of Samples: 8850
Number of Positive Labels: 4047
Number of Negative Labels: 2194
Number of Neutral Labels: 2609
```
Number of positive labels higher than negative and neutral label, thus it might lead to biased model performance, where the model may favor the majority class (positive) and underperform on the minority classes (negative & neutral)
"""

# Apply SMOTE to balance the classes in the training set
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(tfidf_matrix, labels)

print("\nNumber of Samples:", len(data))
print("Number of Positive Labels:", sum(y_train_resampled == 1 ))
print("Number of Negative Labels:", sum(y_train_resampled == -1 ))
print("Number of Neutral Labels:", sum(y_train_resampled == 0 ))

"""## Train-Test Split"""

# To split dataset to train and tes
from sklearn.model_selection import train_test_split
# Check the class distribution after resampling
from collections import Counter

X_train, X_test, y_train, y_test = train_test_split(
    X_train_resampled, y_train_resampled, test_size=0.2, random_state=42
)

print("Training Set Shape:", X_train.shape)
print("Testing Set Shape:", X_test.shape)

resampled_distribution = Counter(y_train)
print("Class distribution after resampling:")
print(resampled_distribution)

"""# MODEL TRAINING & EVALUATION"""

from sklearn.metrics import f1_score, classification_report, accuracy_score
from sklearn.model_selection import GridSearchCV

"""## Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB

# Train the Naive Bayes model
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

# Make predictions
y_pred_nb = nb_model.predict(X_test)

"""## Support Vector Machine"""

from sklearn.svm import LinearSVC

# Train the SVM model
svm_model = LinearSVC(random_state=42)
svm_model.fit(X_train, y_train)

# Predictions and decision scores
y_pred_svm = svm_model.predict(X_test)

"""## Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

# Train the Decision Tree model
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)

# Predictions and probabilities
y_pred_dt = dt_model.predict(X_test)

# Define a function to calculate Accuracy and F1-Score
def evaluate_model(model_name, y_test, y_pred):
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')  # Consistent usage of weighted F1-Score
    print(f"\n{model_name} Evaluation:")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"F1-Score: {f1:.2f}")
    print(classification_report(y_test, y_pred))
    return accuracy, f1


# Evaluate Models
acc_nb, f1_nb = evaluate_model("Naive Bayes", y_test, y_pred_nb)
acc_svm, f1_svm = evaluate_model("Support Vector Machine", y_test, y_pred_svm)
acc_dt, f1_dt = evaluate_model("Decision Tree", y_test, y_pred_dt)

model_results = pd.DataFrame({
    "Model": ["Naive Bayes", "SVM", "Decision Tree"],
    "Accuracy": [acc_nb, acc_svm, acc_dt],
    "F1-Score": [f1_nb, f1_svm, f1_dt]
})

print("\nSummary of Model Performance:")
print(model_results)

"""## Cross Validation and Hyperparameter Tuning on Best Model (SVM)

---


"""

# Define the parameter grid for LinearSVC
param_grid = {
    'C': [0.01, 0.1, 1, 10],  # Regularization strength
    'max_iter': [1000, 5000]  # Maximum number of iterations
}

# Initialize the SVM model
svm_model = LinearSVC(random_state=42)

# Initialize GridSearchCV with F1-score as part of scoring metrics
grid_search_svm = GridSearchCV(
    estimator=svm_model,
    param_grid=param_grid,
    cv=5,  # 5-fold cross-validation
    scoring='f1_weighted',  # Scoring changed to F1-Score for better optimization
    verbose=1
)

# Fit GridSearchCV
grid_search_svm.fit(X_train, y_train)

# Display the best parameters and best F1-Score
print("Best Parameters for SVM:", grid_search_svm.best_params_)
print("Best Cross-Validated F1-Score: {:.2f}%".format(grid_search_svm.best_score_ * 100))

# Evaluate the best model on the test set
best_svm_model = grid_search_svm.best_estimator_
y_pred_best_svm = best_svm_model.predict(X_test)

# Final Model Evaluation
print("\nTest Set Evaluation with Tuned SVM:")
print(classification_report(y_test, y_pred_best_svm))
test_accuracy = accuracy_score(y_test, y_pred_best_svm)
test_f1_score = f1_score(y_test, y_pred_best_svm, average='weighted')

# Print Accuracy and F1-Score
print(f"SVM Accuracy: {test_accuracy:.2f}")
print(f"SVM F1-Score: {test_f1_score:.2f}")

"""## Testing the sentiment prediction using Best Model and TFIDF Vectorizer"""

import re # Regular, to peform text cleaning (remove urls, special characters and extra spaces)
import nltk # NLTK is used for NLP process mainly in text preprocessing (Tokenization, stopword removal, lemmatization, POS taagging)

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk import pos_tag
from contractions import fix #To expand contracted words like "can't" to "cannot".

# Download necessary NLTK data
nltk.download('punkt_tab') # for tokenizing sentences or words
nltk.download('punkt')
nltk.download('wordnet') # For Wordnet, used in lemmatization to access synonyms, definations, and for lemmatization
nltk.download('omw-1.4')  # WordNet multilingual extensions, to support multilingual
nltk.download('averaged_perceptron_tagger_eng') # For part-of-speech tagging
nltk.download('stopwords') # For stopword removal during text preprocessing

# List of custom stopwords to retain from removing, including "no" and "not"
default_stopwords = set(stopwords.words('english'))
custom_stopwords = default_stopwords - {'no', 'not', 'nor', 'none', 'never', 'neither', 'without', 'against',
                                        'but', 'however', 'though', 'although',
                                        'because', 'since', 'due to', 'with'}


# Words that should not be lemmatized
non_lemmatizable_words = {'iphone16', 'iphone16plus', 'iphone16pro', 'iphone16promax', "ios", 'iphone15', 'iphone15plus', 'iphone15pro', 'iphone15promax',
                          'samsunggalaxy23', 'samsunggalaxys23plus', 'samsunggalaxys23ultra', 'samsunggalaxy24', 'samsunggalaxys24plus', 'samsunggalaxys24ultra',
                          'ios17', 'ios18', 'dynamic island', 'a17bionic', 'a18chip', 'usb-c', 'lightning port', 'pro motion', 'ceramic shield',
                          'snapdragon', 'exynos', '120hz', 'amozed', 'one ui',
                          '5g', 'refresh rate', 'fast charging', 'screen size'
                          }

# Function to map POS tags to WordNet tags
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # Default to noun

# Function for text preprocessing
def preprocess_text(text):
    # Remove mentions
    text = re.sub(r'@\w+', '', text)
    # Expand contractions (assuming `fix()` function is defined elsewhere)
    text = fix(text)
    # Remove URLs
    text = re.sub(r'http\S+|www\.\S+', '', text)
    # Remove special characters and punctuation, but keep numbers and dots
    text = re.sub(r'[^a-zA-Z0-9.\s]', '', text)
    # Convert to lowercase
    text = text.lower()
    # Remove redundant whitespaces
    text = re.sub(r'\s+', ' ', text).strip()

    # Tokenize and POS tag
    tokens = word_tokenize(text)
    pos_tags = pos_tag(tokens)
    lemmatizer = WordNetLemmatizer()

    # Enhanced: Edge case handling for empty text
    if not tokens:
        return ''  # Return an empty string if no valid tokens remain

    # Processing tokens with enhanced handling
    processed_tokens = [
        lemmatizer.lemmatize(word, get_wordnet_pos(tag))
        if word.isalpha() and word not in custom_stopwords and word not in non_lemmatizable_words
        else word
        for word, tag in pos_tags
    ]

    return ' '.join(processed_tokens)

# Function to predict sentiment for new tweets with preprocessing
def predict_tweet_sentiment(tweet_text):
    cleaned_text = preprocess_text(tweet_text)
    tweet_vectorized = tfidf_vectorizer.transform([cleaned_text])
    prediction = best_svm_model.predict(tweet_vectorized)
    sentiment_mapping_reverse = {1: 'Positive', 0: 'Neutral', -1: 'Negative'}
    return sentiment_mapping_reverse[prediction[0]]

# Example predictions with preprocessing
new_tweet_1 = "iphone has no security and very ugly design"
new_tweet_2 = "iphone15 is affordable"
new_tweet_3 = "iphone is very expensive"

predicted_sentiment_1 = predict_tweet_sentiment(new_tweet_1)
predicted_sentiment_2 = predict_tweet_sentiment(new_tweet_2)
predicted_sentiment_3 = predict_tweet_sentiment(new_tweet_3)

print("\nNew Tweet Sentiment Predictions:")
print(f"Tweet: {preprocess_text(new_tweet_1)}")
print(f"Predicted Sentiment: {predicted_sentiment_1}")
print(f"\nTweet: {preprocess_text(new_tweet_2)}")
print(f"Predicted Sentiment: {predicted_sentiment_2}")
print(f"\nTweet: {preprocess_text(new_tweet_3)}")
print(f"Predicted Sentiment: {predicted_sentiment_3}")

"""## Model Saving and Loading"""

import joblib
# Save the model and vectorizer
joblib.dump(best_svm_model, 'svm_model.pkl')
joblib.dump(tfidf_vectorizer, 'tf_idfvectorizer.pkl')
print("Model and vectorizer saved successfully.")